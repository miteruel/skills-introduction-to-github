{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Algoritmos de Machine learning \n",
    "\n",
    "Hay muchos tipos de **algoritmos de machine learning** o también se puede decir que hay muchos tipos de **redes neuronales de aprendizage automático**. \n",
    "Segun el tipo de problema es mejor usar una u otra. \n",
    "\n",
    "Para problemas mas complejos tendriamos que combinar varias de ellas.\n",
    "\n",
    "De momento lo importante a entender es que La lista es muy grande, y estan basados en modelos matematicos bastante complejos.\n",
    "\n",
    "## Lista de algoritmos famosos. \n",
    "\n",
    "* Regresión\n",
    "    * Regresión Lineal\n",
    "    * Regresión Logística\n",
    "* MLP: Multi Layered Perceptron – Ejemplo pronóstico de series temporales\n",
    "\n",
    "* Basados en Instancia. Instance-Based.\n",
    "    * k-Nearest Neighbor (kNN) \n",
    "    * ~~Self-Organizing Map~~\n",
    "      \n",
    "* Arbol de Decisión. Decision Tree.\n",
    "    * Arboles de Clasificación y Regresión (CART)\n",
    "    * Decisión de Arbol condicional\n",
    "    * Random Forest\n",
    "\n",
    "* Bayesianos\n",
    "    * Naive Bayes\n",
    "    * Gaussian Naive Bayesn\n",
    "    * Multinomial Naive Bayes\n",
    "    * Bayesian Network\n",
    "      \n",
    "*  Clustering (agrupación) \n",
    "    *  K-Meanso\n",
    "    *  K-Median\n",
    "    *  Hierarchical Cluster\n",
    " \n",
    "* Algoritmos de Aprendizaje Profundo. Deep Learning\n",
    "    * Convolutional Neural Networks \n",
    "    * Long Short Term Memory Neural Networks\n",
    "\n",
    "* Algoritmos de Reducción de Dimensión . CPA.\n",
    "    * Principal Component Analysis\n",
    "    * t-SNE\n",
    "\n",
    "* Procesamiento del Lenguaje Natural (NLP)\n",
    "    * Transformers\n",
    "\n",
    "---\n",
    "\n",
    "El proceso de entrenamiento pude variar bastante entre diferentes redes o algoritmos neuronales. Pero tienen muchas cosas en comun.\n",
    "\n",
    "![image.png](https://analyticsindiamag.com/wp-content/uploads/2018/12/nural-network-banner.gif)\n",
    "\n",
    "Las redes suelen tener varias capas (layers) . Cada capa puede tener muchas neuronas.\n",
    "\n",
    "Las neuronas de cada capa estan 'conectadas' virtualmente a todas las neuronas de la siguiente capa y a las anteriores. La importancia de esa conexion la representa un peso que suele ser un vector de numeros. Tambien hay otro elemento que le llaman  sesgo o bias que matematicamente actua como un normalizador matricial para redes no lineales. Y en una red lineal seria el termino independiente. f(x)= x*2 + a. En este caso a es el sesgo y 2 el peso.\n",
    "\n",
    "Todo esto y mucho mas , ya lo sabe Tensorflow. Puede parecer magia lo que hace ,  porque son objetos muy sofisticados y de muchos tipos. Ademas hay muchos tipos de redes, muchos tipos de capa, de neuronas, etc...\n",
    "\n",
    "Pero ¿que es lo importante? ¿ como funciona una red ? ¿como aprende? \n",
    "Por eso vamos a hacer una red de juguete, muy muy sencilla, pero que ya tenga cosas que van a ser importantes en redes más serias y reales.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los algoritmos de Regresion Lineal.\n",
    "\n",
    "Es uno de los tipos de red más sencillos ya que se trata encontrar una recta optima a unos datos de entrenamiento,\n",
    "con el fin de extrapolar el resto de numeros.\n",
    "\n",
    "## Red de juguete, muy muy sencilla.\n",
    "\n",
    "Nuestra Red es  de 3 capas:\n",
    "\n",
    "* entrada: 1 neurona en la primera capa de entrada\n",
    "* layer: 2 neuronas en la segunda\n",
    "* salida: 1 neurona en la de salida\n",
    "\n",
    "Al ser una red de juguete no podemos cambiar las capas o el numero de neuronas facilmente, porque el codigo esta hecho muy a proposito de esta combinacion.\n",
    "Las neuronas tienen unos pesos y unos sesgos . En el ejemplo estan representados de forma muy basica\n",
    "\n",
    "Una red siempre tiene que tener una capa de entrada y otra de salida. Las que hay en medio se les suele llamar capas ocultas.\n",
    "\n",
    "Aunque no voy a usar TensorFlow ni otras librerias complejas, si que usare numpy porque vamos a hacer operaciones vectoriales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as npy\n",
    "import random\n",
    "\n",
    "#multiplica 2 matrices\n",
    "def mulv (v1,v2):\n",
    "    return npy.dot(v1, v2)\n",
    "\n",
    "# crea una matriz de 2 dimensiones d1xd2  con  numeros aleatorios reles. Pueden ser positivos o negativos. pero con una desviacion estandar pequeña\n",
    "def randomv (d1,d2):\n",
    "    return npy.random.randn(d1, d2)\n",
    "\n",
    "# esta es la formula que queremos imitar\n",
    "def celsius_to_fahrenheit(celsius):\n",
    "    return (celsius * 9 / 5) + 32\n",
    "\n",
    "\n",
    "def celsius_to_Kelvin(celsius):\n",
    "    return celsius+272.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Tasa de aprendizage. Cuanto mas pequeña es puede ser mas precisa, pero puede costar mas entrenamiento\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "class RedNeuronalJuguete:\n",
    "\n",
    "    def __init__(self,lr=LEARNING_RATE):\n",
    "        self.learning_rate=lr\n",
    "        \n",
    "        self.weights1 = randomv(1, 1) ##  pesos de la entrada hasta la layer1\n",
    "        self.sesgos1 = randomv(1, 2)  ## sesgos ,de la segunda capa \n",
    "        self.weights2 = randomv(2, 1) ## pesos  de la segunda capa  con la ultima capa        \n",
    "        self.sesgos2 = randomv(1, 1) ## sesgos(bias) ,de la tercera capa \n",
    "        print('Init',self.weights1,self.sesgos1,self.weights2,self.sesgos2)\n",
    "\n",
    "    # Ejecuta la red. Con unos valores de entrada calcula las salidas de las capas\n",
    "    # Inicialmente dara valores disparatados porque los pesos no estan ajustados y son eleatorios\n",
    "    def forward(self,values):        \n",
    "        output1 = mulv(values, self.weights1) + self.sesgos1        \n",
    "        output2 = mulv(output1, self.weights2) + self.sesgos2\n",
    "        #print ('forward',values,'o1',output1,output2)         \n",
    "        return output1, output2\n",
    "\n",
    "    # calcula la perdida, es decir cuanto error hay en la salida de la red respecto unos valores conocidos\n",
    "    def lost(self,output,respuestaPrueba):\n",
    "        lonv=len(respuestaPrueba)       \n",
    "        loss = npy.square(output - respuestaPrueba).sum() / lonv\n",
    "        return  loss\n",
    "    \n",
    "    # hacer la prepropagacion, sabiendo el resultado final de la red, intenta calcular diferencialmente los sesgos y pesos anteriores\n",
    "    def backward(self,output1, output2,valorPrueba,respuestaPrueba):\n",
    "        grad_output2 = 2 * (output2 - respuestaPrueba) / len(valorPrueba)\n",
    "        grad_weights_2 =mulv(output1.T, grad_output2)\n",
    "        grad_biases_2 = npy.sum(grad_output2)    \n",
    "        grad_output1 = mulv(grad_output2, self.weights2.T)\n",
    "        grad_weights_1 =mulv(valorPrueba.T, grad_output1)\n",
    "        grad_biases_1 = npy.sum(grad_output1)\n",
    "    \n",
    "        return grad_weights_1, grad_biases_1, grad_weights_2, grad_biases_2\n",
    "\n",
    "    # actualizar los pesos y sesgos modificando solo un poco cada  parametro en cada iteracion de entrenamiento\n",
    "    def update_parameters(self,grad_weights_1, grad_biases_1, grad_weights_2, grad_biases_2):\n",
    "       \n",
    "        amount_to_be_updated_wights2 = (self.learning_rate * grad_weights_2)\n",
    "        amount_to_be_updated_sesgos2 = (self.learning_rate * grad_biases_2)\n",
    "        amount_to_be_updated_wights1 = (self.learning_rate * grad_weights_1)\n",
    "        amount_to_be_updated_sesgos1 = (self.learning_rate * grad_biases_1)\n",
    "    \n",
    "        self.weights2 = self.weights2 - amount_to_be_updated_wights2\n",
    "        self.sesgos2 = self.sesgos2 - amount_to_be_updated_sesgos2\n",
    "        self.weights1 =self.weights1 - amount_to_be_updated_wights1\n",
    "        self.sesgos1 = self.sesgos1 - amount_to_be_updated_sesgos1\n",
    "\n",
    "    '''    \n",
    "        Estamos ejecutando 2000 iteraciones de entrenamiento . Cada iteración está hecha de:        \n",
    "        Adelante --> Forward\n",
    "        pasea hacia atrás --> backward\n",
    "        ajuste de parametros --> update_parameters \n",
    "    '''\n",
    "    def fit(self,valorPrueba,respuestaPrueba,nfit=2000,lr=0):\n",
    "        loss=0\n",
    "\n",
    "        if lr>0:\n",
    "            self.learning_rate=lr\n",
    "            \n",
    "        error=''\n",
    "        for t in range(nfit):\n",
    "            output1, output2 = self.forward(valorPrueba)  # calcula los resultados de la red\n",
    "            # hace la backpropagation\n",
    "            grad_weights_1, grad_biases_1, grad_weights_2, grad_biases_2 = self.backward(output1, output2,valorPrueba,respuestaPrueba)\n",
    "            # actualiza pesos y sesgos             \n",
    "            self.update_parameters(grad_weights_1, grad_biases_1, grad_weights_2, grad_biases_2)\n",
    "            # calcula que perdida tiene actualmente y si es pequeña deja de entrenar\n",
    "            loss=self.lost(output2,respuestaPrueba)\n",
    "            if loss<0.05:\n",
    "                print (loss)\n",
    "                break\n",
    "            if t%500==0:\n",
    "                print ('fit epoca:',t,'precision',loss)\n",
    "            \n",
    "            ok=True\n",
    "            for n in self.weights2:\n",
    "                if npy.isnan(n):\n",
    "                    print('nan')\n",
    "                    ok=False\n",
    "                    break\n",
    "            if not ok: \n",
    "                error='NaN resultado'\n",
    "                break\n",
    "        return loss,error\n",
    "\n",
    "    # predice una lista de valores\n",
    "    def predicted_values(self,valores):\n",
    "        output1, prediction = self.forward(valores)\n",
    "        return prediction\n",
    "    \n",
    "    # predice un valor\n",
    "    def predicted(self,valor):\n",
    "        cel_value=[[valor]]\n",
    "        cv=npy.array(cel_value)\n",
    "        #print('prediccion',cel_value,cv)\n",
    "        output1, prediction = self.forward(cv)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    # guarda los datos del modelo entrenado\n",
    "    def save(self,nf):\n",
    "        fi=open(nf,'w')\n",
    "        fi.write (toStrv(self.weights1)+'\\n')\n",
    "        fi.write (toStrv(self.sesgos1)+'\\n')\n",
    "        fi.write (toStrv(self.weights2)+'\\n')\n",
    "        fi.write (toStrv(self.sesgos2)+'\\n')\n",
    "\n",
    "    # carga los datos del modelo entrenado\n",
    "    def load(self,nf):\n",
    "        vectores=[]\n",
    "        fi=open(nf,'r')\n",
    "        for s in fi:\n",
    "            v=eval(s)\n",
    "            vectores.append(v)\n",
    "        self.weights1=npy.array(vectores[0])\n",
    "        self.sesgos1=npy.array(vectores[1])\n",
    "        self.weights2=npy.array(vectores[2])\n",
    "        self.sesgos2=npy.array(vectores[3])\n",
    "        \n",
    "# funcion helper para save. rederiza el vector numpy en una string\n",
    "def toStrv (vector):    \n",
    "    lista_python =vector.tolist()\n",
    "    s=str(lista_python)   \n",
    "    return s.replace(\" \", \"\",len(lista_python))\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creamos un objeto redneuronal.\n",
    "* Preparamos datos de entrenamiento\n",
    "* Entrenamos la red\n",
    "* salvamos el resultado de entrenamiento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red=RedNeuronalJuguete()\n",
    "celsius    = npy.array([[-40], [-10], [0], [8], [15], [22], [38]], dtype=float)\n",
    "fahrenheit = npy.array([[-40], [14],  [32],  [46],  [59],  [72],  [100]], dtype=float)\n",
    "\n",
    "red.fit(celsius,fahrenheit,5000)\n",
    "red.save('celfah.modeljuguete')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenada la red podemos hacer predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_values = red.predicted_values(celsius)\n",
    "grados=22;\n",
    "valor=red.predicted(grados)\n",
    "print ('celsius',grados,'predice fahrenheit',valor,'real',celsius_to_fahrenheit(grados))\n",
    "#print ('predice',valor,red.weights1,red.sesgos1,red.weights2,red.sesgos2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(celsius, fahrenheit, '-b', label='real')\n",
    "#ax.plot(celsius_values, predicted_before_training, '--r', label='Predicted before Training')\n",
    "ax.plot(celsius, predicted_values, '--g', label='Predictivo despues de entrenar')\n",
    "plt.xlabel(\"Celsius\")\n",
    "plt.ylabel(\"Fahrenheit\")\n",
    "ax.legend(frameon=False)\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora podemos crer otro objeto red, cargar los pesos de entrenamiento y usarlo sin mas, sin entrenarlo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2=RedNeuronalJuguete()\n",
    "red2.load('celfah.modeljuguete')\n",
    "valor=red2.predicted(22)\n",
    "print ('red2 predice',valor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celsius a Kelvin\n",
    "Tambien podemos usar el mismo metodo para entrenar una red para calcular grados Kelvin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redk=RedNeuronalJuguete()\n",
    "celsius_k    = npy.array([[-50. ], [-37.5], [-25. ], [-12.5],  [  0. ], [ 12.5], [ 25. ], [ 37.5],[ 50. ]], dtype=float)\n",
    "kelvin = npy.array( [[222.15],[234.65],[247.15],[259.65],[272.15], [284.65], [297.15], [309.65], [322.15]], dtype=float)\n",
    "\n",
    "redk.fit(celsius_k,kelvin,7000)\n",
    "redk.save('celkel.modeljuguete')\n",
    "redk=None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando una red está entrenada, ya no necesitariamos el codigo complicado de entrenamiento. Podemos usar una version simplificada de codigo que simplemente haga las operaciones de calculo sobre los pesos y sesgos entrenados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RuntimeRedJuguete:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weights1 = randomv(1, 1) \n",
    "        self.sesgos1 = randomv(1, 2) \n",
    "        self.weights2 = randomv(2, 1)\n",
    "        self.sesgos2 = randomv(1, 1) \n",
    "        \n",
    "    def forward(self,values):        \n",
    "        output1 = mulv(values, self.weights1) + self.sesgos1        \n",
    "        output2 = mulv(output1, self.weights2) + self.sesgos2        \n",
    "        return output1, output2\n",
    "    \n",
    "    def load(self,nf):\n",
    "        vectores=[]\n",
    "        fi=open(nf,'r')\n",
    "        for s in fi:\n",
    "            v=eval(s)\n",
    "            vectores.append(v)\n",
    "        self.weights1=npy.array(vectores[0])\n",
    "        self.sesgos1=npy.array(vectores[1])\n",
    "        self.weights2=npy.array(vectores[2])\n",
    "        self.sesgos2=npy.array(vectores[3])\n",
    "        \n",
    "    def predicted_values(self,valores):\n",
    "        output1, prediction = self.forward(valores)\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    def predicted(self,valor):\n",
    "        cel_value=[[valor]]\n",
    "        cv=npy.array(cel_value)\n",
    "        #print('prediccion',cel_value,cv)\n",
    "        output1, prediction = self.forward(cv)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "red=RuntimeRedJuguete()\n",
    "red.load('celkel.modeljuguete')\n",
    "\n",
    "predicted_vector = red.predicted_values(celsius_k)\n",
    "for i in range (10):\n",
    "    n=random.randint(-100,100)\n",
    "    valor=red.predicted(n)\n",
    "    k=float(valor)\n",
    "    print ('predice',n,valor,k,'real',celsius_to_Kelvin(n))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(celsius_k, kelvin, '-b', label='real')\n",
    "#ax.plot(celsius_values, predicted_before_training, '--r', label='Predicted before Training')\n",
    "ax.plot(celsius_k, predicted_vector, '--g', label='Predictivo despues de entrenar')\n",
    "plt.xlabel(\"Celsius\")\n",
    "plt.ylabel(\"kelvin\")\n",
    "ax.legend(frameon=False)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crea una Red Neuronal de regresion logistica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Red Neuronal de regresion logistica\n",
    "\n",
    "En este tipo de redes se operan con datos discretos, no continuos de una regresion lineal.\n",
    "A partir de un conjunto de datos de entrada, da un resultado binario 0 o 1. O mejor dicho un valor aproximado como 0.096 para el 0 o 0.887 para el 1.\n",
    "Cuanto más proximo esté al 0 o al 1, mas segura es la respuesta.\n",
    "\n",
    "Tampoco tiene que ser un unico dato binario. Puede ser un array de respuestas binarias. En este caso suele ser una clasificacion.\n",
    "\n",
    "EN la regresión logistica aparece un nuevos conceptos importantes \n",
    "\n",
    "# Función de activación.\n",
    "\n",
    "* Es la funcion que actua sobre los pesos y sesgos de la neurona para activarla o no. Imitando un comportamiento casi binario.\n",
    "\n",
    "  *  https://es.wikipedia.org/wiki/Función_de_activación\n",
    "  *  https://es.wikipedia.org/wiki/Perceptrón\n",
    " \n",
    "  Se suele usar mucho la funcion sigmoide f(x)=1+e−x1​\n",
    "\n",
    "![image.png](https://www.i2tutorials.com/wp-content/media/2019/11/Sigmoid-function-i2tutorials.gif)\n",
    "  \n",
    " * Esta función toma cualquier valor real y lo transforma en un valor entre 0 y 1. \n",
    " * Saturación: Para valores muy grandes o muy pequeños de (x), la salida de la función se aplana y se aproxima a 1 o 0, respectivamente.\n",
    " * Derivada simple: La derivada de la función sigmoide es fácil de calcular, lo que facilita el proceso de retropropagación en el entrenamiento de redes neuronales.\n",
    " * No linealidad: Introduce no linealidad en la red, permitiendo que la red neuronal aprenda relaciones complejas entre las entradas y las salidas.\n",
    "\n",
    "# función de pérdida\n",
    "\n",
    "Todas las redes neuronales utilizan una función de pérdida que cuantifica el error entre la salida predicha y la verdad de una muestra de entrenamiento determinada. La función de pérdida se puede utilizar para guiar el proceso de aprendizaje (es decir, actualizar los pesos de la red de una manera que mejore la precisión de las predicciones futuras). \n",
    "\n",
    "# descenso de gradiente \n",
    "Conociendo esas perdidas, las redes usan una tecnica llamada  descenso de gradiente (Gradient_descent) para optimizar la localización de puntos criticos. https://es.wikipedia.org/wiki/Descenso_del_gradiente\n",
    "\n",
    "![image.jpg](https://mlpills.dev/wp-content/uploads/2022/10/CaIB7lz-h.jpg)\n",
    "\n",
    "Crearemos otra red neuronal de juguete de regresion logistica,  con 3 capas, y con valores de entrada/salida -1 a 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T10:30:57.447541Z",
     "start_time": "2018-07-25T10:30:56.855041Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    #return 1.0/(1.0 + math.exp(-x))\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivada(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivada(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, activation='tanh'):\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_prime = sigmoid_derivada\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_prime = tanh_derivada\n",
    "\n",
    "        # inicializo los pesos\n",
    "        self.weights = []\n",
    "        self.deltas = []\n",
    "        # capas = [2,3,2]\n",
    "        # rando de pesos varia entre (-1,1)\n",
    "        # asigno valores aleatorios a capa de entrada y capa oculta\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # asigno aleatorios a capa de salida\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Agrego columna de unos a las entradas X\n",
    "        # Con esto agregamos la unidad de Bias a la capa de entrada\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        #print (ones)\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # Calculo la diferencia en la capa de salida y el valor obtenido\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "            \n",
    "            # Empezamos en el segundo layer hasta el ultimo\n",
    "            # (Una capa anterior a la de salida)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "            self.deltas.append(deltas)\n",
    "\n",
    "            # invertir\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiplcar los delta de salida con las activaciones de entrada \n",
    "            #    para obtener el gradiente del peso.\n",
    "            # 2. actualizo el peso restandole un porcentaje del gradiente\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 1000 == 0: print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "        ones = np.atleast_2d(np.ones(x.shape[0]))\n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "    def print_weights(self):\n",
    "        print(\"LISTADO PESOS DE CONEXIONES\")\n",
    "        for i in range(len(self.weights)):\n",
    "            print(self.weights[i])\n",
    "\n",
    "    def get_deltas(self):\n",
    "        return self.deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una Primer Red emulando a la función XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T10:30:58.202079Z",
     "start_time": "2018-07-25T10:30:57.449918Z"
    }
   },
   "outputs": [],
   "source": [
    "# funcion XOR\n",
    "nn = NeuralNetwork([2,2,1])\n",
    "X = np.array([[0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "nn.fit(X, y,epochs=8000)\n",
    "for e in X:\n",
    "    print(\"Entrdas:\",e,\"Salidas:\",nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficamos la función coste "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos como el gradiente desciende y disminuye el error a medida que pasan las iteraciones de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T10:31:40.473536Z",
     "start_time": "2018-07-25T10:31:40.183342Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "deltas = nn.get_deltas()\n",
    "\n",
    "valores=[]\n",
    "index=0\n",
    "for arreglo in deltas:\n",
    "    valores.append(arreglo[1][0] )\n",
    "    index=index+1\n",
    "\n",
    "plt.plot(range(len(valores)), valores, color='b')\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow\n",
    "\n",
    "Veamos los mismos ejemplos usando tensorflow.\n",
    "Keras es una parte de tensorflow. Facilita la creacion y entrenamiento de modelos de red neuranal\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# datos de entrenamiento\n",
    "celsius = np.array([-40, -10, 0, 8, 15, 22, 38], dtype=float)\n",
    "fahrenheit = np.array([-40, 14, 32, 46, 59, 72, 100], dtype=float)\n",
    "\n",
    "# aumentar las epocas de aprendizaje suele dar mas precisión\n",
    "epocas =400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una sola neurona\n",
    "Una sola neurona es suficiente para un calculo lineal simple. Pero va a ser poco precisa o le cuesta más epocas entrenarse\n",
    "Esta neurona es la capa de salida ya que la entrada siempre es implicita. \n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primer ejemplo de un modelo de solo una capa y 1 neurona. \n",
    "# Un modelo Dense en un modelo simple en el que las neuronas de una capa estan conectadas a todas de la siguiente\n",
    "capa = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
    "# creamos el modelo como una secuencia de una sola capa\n",
    "modelo = tf.keras.Sequential([capa])\n",
    "\n",
    "# prepara el modelo con una funcion de perdida de minimo cuadrado, y un descenso de gradiente de tipo Adam\n",
    "# optimizer=tf.keras.optimizers.Adam(0.1),\n",
    "# learning_rate=0.001 optimizer='adam',\n",
    "modelo.compile(\n",
    "    # podemos aumentar el ratio de aprendizaje, con riesgo a ser menos precisos\n",
    "    tf.keras.optimizers.Adam(0.2),\n",
    "    \n",
    "    loss='mean_squared_error'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comenzando entrenamiento...\")\n",
    "historial = modelo.fit(celsius, fahrenheit, epochs=epocas, verbose=True)\n",
    "print(\"Modelo entrenado!\",str(historial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"# Epoca\")\n",
    "plt.ylabel(\"Magnitud de pérdida\")\n",
    "plt.plot(historial.history[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hagamos una predicción!\")\n",
    "resultado = modelo.predict( np.array([20.0]))\n",
    "print(\"El resultado es \" + str(resultado) + \" fahrenheit!\")\n",
    "print(\"Variables internas del modelo\")\n",
    "print(capa.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ahora hagamos otro modelo con varias capas sobre el mismo problema.\n",
    "LLega a resultados mas precisos con el mismo entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epocas =300\n",
    "\n",
    "oculta1 = tf.keras.layers.Dense(units=3, input_shape=[1])\n",
    "oculta2 = tf.keras.layers.Dense(units=3)\n",
    "salida = tf.keras.layers.Dense(units=1)\n",
    "modelo2 = tf.keras.Sequential([oculta1, oculta2, salida])\n",
    "\n",
    "\n",
    "modelo2.compile(\n",
    "    optimizer=   tf.keras.optimizers.Adam(0.02),\n",
    "\n",
    "    loss='mean_squared_error'\n",
    ")\n",
    "\n",
    "print(\"Comenzando entrenamiento...\")\n",
    "historial2 = modelo2.fit(celsius, fahrenheit, epochs=epocas, verbose=True)\n",
    "print(\"Modelo entrenado!\",str(historial2))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"# Epoca\")\n",
    "plt.ylabel(\"Magnitud de pérdida\")\n",
    "plt.plot(historial2.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hagamos una predicción!\")\n",
    "resultado = modelo2.predict( np.array([20.0]))\n",
    "print(\"El resultado es \" + str(resultado) + \" fahrenheit!\")\n",
    "print(\"\\nVariables internas del modelo\")\n",
    "print(oculta1.get_weights())\n",
    "print(oculta2.get_weights())\n",
    "print(salida.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# serializar el modelo a JSON\n",
    "def salvar_modelo (model,nf):\n",
    "\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(nf+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    model.save_weights(nf+\".weights.h5\")\n",
    "    print(\"Modelo Guardado!\")\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "# cargar json y crear el modelo\n",
    "def load_modelo(nf):\n",
    "    json_file = open(nf+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # cargar pesos al nuevo modelo\n",
    "    loaded_model.load_weights(nf+\".weights.h5\")\n",
    "    print(\"Cargado modelo desde disco.\")\n",
    "    return loaded_model\n",
    "\n",
    "salvar_modelo(modelo2,'celsius_fahrenheit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_modelo('celsius_fahrenheit')\n",
    "\n",
    "print(\"Hagamos una predicción!\")\n",
    "resultado = modelo2.predict( np.array([20.0]))\n",
    "print(\"El resultado es \" + str(resultado) + \" fahrenheit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "# cargamos las 4 combinaciones de las compuertas XOR\n",
    "training_data = np.array([[0,0],[0,1],[1,0],[1,1]], \"float32\")\n",
    "test_data = np.array([[0,0]], \"float32\")\n",
    " \n",
    "# y estos son los resultados que se obtienen, en el mismo orden\n",
    "target_data = np.array([[0],[1],[1],[0]], \"float32\")\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "\n",
    " \n",
    "model.fit(training_data, target_data, epochs=300)\n",
    " \n",
    "# evaluamos el modelo\n",
    "scores = model.evaluate(training_data, target_data)\n",
    " \n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "nose=model.predict(test_data).round()\n",
    "print ('nose',nose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tensorflow.rstudio.com/examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celsius_to_fahrenheit(celsius):\n",
    "    return (celsius * 9 / 5) + 32\n",
    "\n",
    "\n",
    "def celsius_to_Kelvin(celsius):\n",
    "    return celsius+272.15\n",
    "\n",
    "SAMPLE_SIZE = 9\n",
    "\n",
    "#LEARNING_RATE = 0.0001\n",
    "\n",
    "\n",
    "def conjunto_celsius_fahrenheit():\n",
    "    celsius_values = npy.linspace(-50, 50, SAMPLE_SIZE)\n",
    "    print(celsius_values)\n",
    "    celsius_values = celsius_values.reshape(SAMPLE_SIZE, 1)\n",
    "    print(celsius_values)\n",
    "    fahrenheit_values = celsius_to_fahrenheit(celsius_values)\n",
    "    #fahrenheit_values = celsius_to_Kelvin(celsius_values)\n",
    "    return celsius_values, fahrenheit_values\n",
    "    #print(fahrenheit_values)\n",
    "\n",
    "\n",
    "def conjunto_celsius_Kelvin():\n",
    "    celsius_values = npy.linspace(-50, 50, SAMPLE_SIZE)\n",
    "    print(celsius_values)\n",
    "    celsius_values = celsius_values.reshape(SAMPLE_SIZE, 1)\n",
    "    print(celsius_values)\n",
    "   \n",
    "    k_values = celsius_to_Kelvin(celsius_values)\n",
    "    return celsius_values, k_values\n",
    "    #print(fahrenheit_values)\n",
    "\n",
    "\n",
    "cv,kv=conjunto_celsius_Kelvin()\n",
    "print (cv,kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
