{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers, ¿qué pueden hacer?\n",
    "Transformers es el nombre de la libreria principal de Huggingface. (hf)\n",
    "\n",
    "Basicamente esta enfocada en facilitar el uso de modelos preentrenados por otros usuarios, o para archivar nuestros propios modelos preentrenados para usarlos en nuestros propios proyectos o compartirlo con otros.\n",
    "\n",
    "Casi todos los modelos que tienen son modelos Transformer, no confundir con el nombre de la libreria en si. aunque la relación es evidente.\n",
    "\n",
    "## checkpoint\n",
    "Un modelo preentrenado se le llama checkpoint. Sabemos que un modelo es un programa que puede aprender en base a entrenarlo con unos datos. \n",
    "\n",
    "El mismo programa modelo puede ser más o menos efectivo dependiendo de la información facilitada al entrenamiento, o el numero de epocas de entrenamiento, etc... Al final el resultado de ese entrenamiento  se guarda como unos 'numeros' que representan pesos y sesgos.  Eso es un checkpoint. \n",
    "\n",
    "Una primera forma de verlo es que despues de x epocas de entrenamiento, guardes los datos de pesos y sesgos aprendidos como 'copia de seguridad' de lo aprendido hasta ese momento. En ese punto podriamos parar el entrenamiento y continuar posteriormente.\n",
    "Si continuamos el entrenamiento con los mismos datos y condiciones que antes, seguiriamos entrenando el modelo como si no hubieramos parado, buscando minificar la funcion de perdida del modelo.\n",
    "\n",
    "Cambiar los datos de entrenamiento o condiciones anteriores puede tener efectos no deseados segun el modelo, sobre todo porque puede 'olvidar' datos anteriores o haber un sobreentrenamiento. \n",
    "\n",
    "Añadir más datos al conjunto de entrenamiento suele tener mas beneficios, pero una vez que has empezado a entrenar un modelo con unos datos, cualquier cambio en las condiciones o datos de entrenamiento puede tener resultados imprevisibles tanto buenos como malos.\n",
    "\n",
    "### fine-tunning\n",
    "La mayoria de los checkpoint admiten un reentrenamiento, al menos parcial, a eso le llaman fine-tunning.\n",
    "Ese finetunning  tambien pueden ser de muchos tipos. Y rara vez consiste en continuar entrenando con mas epocas el chechpoint con los mismos datos o parecidos. \n",
    "\n",
    "Tampoco es trivial tomar un modelo preentrenarlo y aplicarle un nuevo entrenamiento con nuevos datos que nos interesen . Corremos el riesgo que 'desaprenda' cosas anteriores o si insistimos con datos muy concretos haya un sobre-entrenamiento (haria muy bien las cosas con datos muy concretos, pero mal con datos más reales o genericos)\n",
    "\n",
    "Depende mucho del diseño del propio modelo, unos se deberan reentrenar de una manera conjunta y otros de forma parcial. \n",
    "\n",
    "Tampoco es facil tomar un modelo grande y aplicarle una simple epoca mas de entrenamiento. Necesitariamos  recursos de hardware equivalentes como entrenarlos desde 0.\n",
    "\n",
    "### LoRA\n",
    "Una forma muy usada de fine-tunning es con adaptadores LoRA, o Adaptación de Bajo Rango (Low-Rank Adaptation), Se puede ver como un añadido que le hacemos al modelo original sin cambiarlo directamente. Lo de Bajo rango tiene que ver que lo hemos entrenado con unas 'dimensiones' más pequeñas que el modelo principal, (menos datos o menos dimensiones de los pesos y sesgos, etc). \n",
    "El modelo debe estar diseñado para admitir esos adaptadores LoRA, es decir no se puede hacer con todos los modelos.\n",
    "\n",
    "\n",
    "\n",
    "HF Transformer facilita el hacer estos fine-tunning de modelos basicos con un entrenamiento generico o hacer adaptadores especializados para modelos abiertos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instala Transformers, Datasets, y Evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para poderse loguear a hf desde el notebook\n",
    "from huggingface_hub import notebook_login\n",
    "# quitar comentario para hace login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## pipeline\n",
    "pipeline() es la forma más fácil de usar un modelo preentrenado para una tarea dada.\n",
    "El pipeline() soporta muchas tareas comunes y modelos listos para usar:\n",
    "\n",
    "La palabra pipeline, tuberia, hace alusión a que crea un flujo de informacion  que pasa por la tuberia de procesos y sale un resultado.\n",
    "\n",
    "La tarea mas sencilla seria hacer inferencia con un modelo. Otras tareas mas complejas serian entrenar un modelo, testear un modelo, hacer un fine-tunning, etc..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\python\\python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python\\python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9366181492805481}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Importa pipeline() , especificando el tipo de modelo a usar:\n",
    "clasificador = pipeline(\"sentiment-analysis\", device=-1) # -1= CPU # device=0 (cuda gpu)\n",
    "clasificador(\"me gusta la gente que  comprende HuggingFace \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El pipeline descarga y almacena en caché el modelo preentrenado y tokeniza para análisis de sentimiento. \n",
    "Como no hemos expecificado un  modelo concreto el pipeline 'elige'  uno por defecto.\n",
    "al ser un modelo generico con diccionario ingles, quizas no es bueno clasificando frases más complicadas en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9380154013633728}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador(\"casi termino de comprender lo bueno que son los transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos especificar un modelo preentrenado o checkpoint diferente al de por defecto. \n",
    "Pueden estar basados en diferentes modelos o son fine tunings distintos del mismo modelo .\n",
    "\n",
    "En cualquier caso todos los modelos que sean del grupo sentiment-analysis se pueden usar de forma muy similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "clasificador = pipeline(\"sentiment-analysis\", model=\"pysentimiento/robertuito-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.8589496612548828}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador(\"casi termino de comprender lo bueno que son los transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: En este 2 modelo de sentimientos en vez de ser las etiquietas de resultado POSITIVE o NEGATIVE son POS o NEG.\n",
    "Simplemente ilustra que pueden haber pequeñas diferencias entre modelos del mismo tipo. Se parecerán, pero cada uno tendrá unas caracteristicas, concretas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.6889551281929016},\n",
       " {'label': 'NEG', 'score': 0.8335683345794678}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador(\n",
    "    [\" HuggingFace es lo que espero.\", \"odio mucho esto!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como conclusión, un pipeline facilita y simplifica al maximo el uso de un modelo, de forma que abstrae la funcionalidad de los modelos,\n",
    "ocultando los detalles más especificos.\n",
    "Son un buen punto de partida para probar un modelo, y lo normal es que si quieres mejorar o hacer un fine-tunning de alguno de ellos,\n",
    "tienes que estudiar y conocer más detalles.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## partes de un transformer\n",
    "\n",
    "Un transformer tiene 2 partes separadas y pueden ser más o menos independientes. \n",
    "* tokenizador \n",
    "* modelo\n",
    "\n",
    "El tokenizador es el resposable de convertir a numeros los datos de entrada. Los datos pueden ser palabras, imagen, sonido, series...\n",
    "Hay muchas estrategias de tokenizacion, incluso si solo tenemos en cuenta palabras. Es toda una ciencia la tokenicacion de cosas.\n",
    "O incluso la tokenizacion de conceptos multi-modales.\n",
    "\n",
    "### atencion?\n",
    "El modelo es la parte de red neuronal en si. que suelen ser muchas capas de red 'tradicionales' en los que los datos usan un flujo forward/back \n",
    "y intercaladas unas capas llamadas de atencion en la que hay cierto flujo en la propia capa de forma que neuronas más cercanas se influyen \n",
    "con cierto peso que le llaman atención. Esa atencion tambien se puede ver como un tercer miembro del equipo peso , sesgo y atención.\n",
    "\n",
    "El resulado de la inferencia con el modelo, sera de nuevo una lista de numeros, y se usara de nuevo el tokenizador pero a la inversa para \n",
    "convertir esos numeros el palabras (o imagen, o sonido...). \n",
    "Es decir el tokenizador tiene la funcion de codificar y decodificar los datos que entran y salen del modelo\n",
    "    \n",
    "Que me perdonen los cientificos de datos las simplificaciones, omisiones y errores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginemos un texto y queremos dividirlos en palabras sueltas. Una primera forma que podemos hacer es simplemente divididir por espacios la frase completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antonio', 'alcázar', 'likes', 'transformer', 'network', '.', 'And', 'transformer', 'likes', 'attention']\n"
     ]
    }
   ],
   "source": [
    "texto =\"antonio alcázar likes transformer network . And transformer likes attention\"\n",
    "tokenized_text = texto.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podriamos hacer un diccionario con las palabras de esa frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "('.', 0)\n",
      "('And', 1)\n",
      "('alcázar', 2)\n",
      "('antonio', 3)\n",
      "('attention', 4)\n",
      "('likes', 5)\n",
      "('network', 6)\n",
      "('transformer', 7)\n"
     ]
    }
   ],
   "source": [
    "diccionario = sorted(set(tokenized_text))\n",
    "vocab_size = len(diccionario)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(diccionario)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad un diccionario asi es demasiado pequeño para que tenga una utilidad, \n",
    "lo normal es usar un diccionario mucho más grande y que sea más o menos estandar para poderse usar en diferentes modelos.\n",
    "Por otro lado, por más grande que sea un diccionario de palabras siempre podriamos encrontrar una palabra nueva, bien por ser de otro idioma, bien por ser una palabra compuesta o un neologismo. La forma de gestionar esas palabras nuevas es lo que diferencia principalmente los algoritmos de tokenizacion.\n",
    "Tampoco interesa tener diccionarios infinitamente grandes, primero por el propio rendimiento, a mas numeros y datos más lento.Y sobre todo porque hay palabras de igual raiz semantica, por ejemplo vervos, que puede interesar descomponer para usar menos tokens, o evitar tener la misma palabra en singular y plural. O si es una palabra compuesta, masculino/femenino, etc..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ant', '##oni', '##o', 'al', '##c', '##á', '##zar', 'likes', 'transform', '##er', 'network', '.', 'And', 'transform', '##er', 'likes', 'attention']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = texto\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22904, 11153, 1186, 2393, 1665, 5589, 8950, 7407, 11303, 1200, 2443, 119, 1262, 11303, 1200, 7407, 2209]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antonio alcázar likes transformer network. And transformer likes attention\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"pysentimiento/robertuito-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "raw_inputs = [\n",
    "   \" HuggingFace es lo que espero.\", \"odio mucho esto!\"\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# checkpoint = \"pysentimiento/robertuito-sentiment-analysis\"\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "tokenizer.save_pretrained(\"directorio_en_mi_computador\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Análisis de Sentimiento (sentiment-analysis) clasifica la polaridad of un texto dado . The tool clasifies the polaridad and analyzes the texto . Texto is a texto a partir of un input dado. The tool is a tool that generates texto, rather than an algorithm .'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "texto= \"\"\"\n",
    "    Texto: \n",
    "Análisis de Sentimiento (sentiment-analysis): clasifica la polaridad de un texto dado. \n",
    "    \n",
    "Generación de Texto (Text Generation): genera texto a partir de un input dado. \n",
    "Reconocimiento de Entidades (Name Entity Recognition o NER, en inglés): etiqueta cada palabra con la entidad que representa (persona, fecha, ubicación, etc.). Responder Preguntas (Question answering, en inglés): extrae la respuesta del contexto dado un contexto y una pregunta.\n",
    "\n",
    "Rellenar Máscara (Fill-mask, en inglés): rellena el espacio faltante dado un texto con palabras enmascaradas. Resumir (Summarization, en inglés): genera un resumen de una secuencia larga de texto o un documento. Traducción (Translation, en inglés): traduce un texto a otro idioma.\n",
    "\n",
    "Extracción de Características (Feature Extraction, en inglés): crea una representación tensorial del texto.\n",
    "\n",
    "Imagen:\n",
    "\n",
    "Clasificación de Imágenes (Image Classification, en inglés): clasifica una imagen. Segmentación de Imágenes (Image Segmentation, en inglés): clasifica cada pixel de una imagen. Detección de Objetos (Object Detection, en inglés): detecta objetos dentro de una imagen.\n",
    "\n",
    "Audio:\n",
    "\n",
    "Clasificación de Audios (Audio Classification, en inglés): asigna una etiqueta a un segmento de audio. Reconocimiento de Voz Automático (Automatic Speech Recognition o ASR, en inglés): transcribe datos de audio a un texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "summarizer = pipeline(\"summarization\", device=-1)\n",
    "summarizer(texto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#summarizer = pipeline(\"summarization\", model=\"Narrativa/bsc_roberta2roberta_shared-spanish-finetuned-mlsum-summarization\")\n",
    "#summarizer(texto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", device=-1)\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModel.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(predictions)\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", device=-1)\n",
    "generator(\"en este curso vamos a enseñar algo de inteligencia artificial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\", device=-1)\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", device=-1)\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True, device=-1)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", device=-1)\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", device=-1)\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\", device=-1)\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesgos y limitaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\", device=0)\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"cpierse/wav2vec2-large-xlsr-53-esperanto\")\n",
    "model = AutoModelForCTC.from_pretrained(\"cpierse/wav2vec2-large-xlsr-53-esperanto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "test_dataset = load_dataset(\"common_voice\", \"eo\", split=\"test[:2%]\", trust_remote_code=True) \n",
    "processor = Wav2Vec2Processor.from_pretrained(\"cpierse/wav2vec2-large-xlsr-53-esperanto\") \n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"cpierse/wav2vec2-large-xlsr-53-esperanto\") \n",
    "\n",
    "resampler = torchaudio.transforms.Resample(48_000, 16_000)\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the aduio files as arrays\n",
    "def speech_file_to_array_fn(batch):\n",
    "   speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "   batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n",
    "   return batch\n",
    "\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
    "inputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "   logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "print(\"Prediction:\", processor.batch_decode(predicted_ids))\n",
    "print(\"Reference:\", test_dataset[\"sentence\"][:2])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Transformadores, ¿qué pueden hacer?",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
